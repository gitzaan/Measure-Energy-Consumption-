# -*- coding: utf-8 -*-
"""AI_Phase4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1f8qXX_ilKrUpCShkoasRnKLwkSGU35Q0

# __Phase 4: Development Part 2 - Model Training__

**Objective:**

In this phase, we will continue building the "Measure Energy Consumption" project by performing Feature Engineering , Model Training and Evaluation on the dataset obtained by Kaggle.The primary goal is to have a model with high accuracy and integration with other functionalities.




**Team Details:**

 1. College Name: Madras Institute of Technology, Anna University

2. Team Members:

| Name                        | Email                    | NM ID                                |
|-----------------------------|--------------------------|--------------------------------------|
| Fowzaan Abdur Razzaq Rasheed | fowzaan.rasheed@gmail.com  | 8E4AF1FB4D2CAD089814D6BED938AC27     |
| Mohit S                     | smohit28.04@gmail.com     | B80CBC310CADE36AB9A4F5A439515636    |
| Pronoy Kundu                | pronoykundu513@gmail.com  | 11306F001F2B3639BBE4CB15C475F9EC    |

# __Importing Libraries and Setting Up the Environment__
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import tensorflow as tf
import sklearn.preprocessing
from sklearn.metrics import r2_score
from keras.layers import Dense, Dropout, SimpleRNN, LSTM, Bidirectional, Input , RepeatVector
from keras.models import Sequential, Model
from keras.optimizers import Adam
from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping
from sklearn.preprocessing import MinMaxScaler, StandardScaler
from sklearn.model_selection import TimeSeriesSplit
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import math
import os
import seaborn as sns
from google.colab import drive
drive.mount('/content/drive')
print("Libraries imported successfully")

"""# __Loading The Dataset__

The dataset AEP_hourly.csv is loaded using the pandas library's read_csv function. The parse_dates parameter is set to ['Datetime'] to ensure that the 'Datetime' column is recognized as a datetime object. The index_col parameter is set to 'Datetime' to set the 'Datetime' column as the index of the DataFrame.
"""

# Load AEP_hourly.csv
aep_data = pd.read_csv('/content/drive/MyDrive/NM Proj/AEP_hourly.csv', parse_dates=['Datetime'], index_col='Datetime')
print("Data loaded successfully.")
print("AEP Data Shape:", aep_data.shape)

"""# __Data Preprocessing: Handling Missing Values and Outliers in AEP Time Series Data__

**Checking for missing values**

-  We check for missing values in the DataFrame using the isnull().sum() function. If any missing values are found, they are removed using the dropna() function. If no missing values are found, the original DataFrame is used.
"""

# Count missing values
missing_values = aep_data.isnull().sum()

if missing_values.any():
    # Remove rows with missing values
    aep_data_cleaned = aep_data.dropna()

    print("Number of Missing Values:")
    print(missing_values)

    print("Data cleaned successfully.")
    print("AEP Data Shape (Cleaned):", aep_data_cleaned.shape)
else:
    print("No missing values found. Data is already clean.")
    aep_data_cleaned = aep_data

"""**Checking for Outliers**

-  We check for outliers in the 'AEP_MW' column of the DataFrame. Outliers are identified using the Z-score method, where any data point with a Z-score greater than a threshold (set to 3 in this case) is considered an outlier. If any outliers are found, they are removed from the DataFrame
"""

# Count outliers
outlier_threshold = 3  # You can adjust this threshold as needed
z_scores = (aep_data_cleaned['AEP_MW'] - aep_data_cleaned['AEP_MW'].mean()) / aep_data_cleaned['AEP_MW'].std()
outliers = (z_scores.abs() > outlier_threshold)
num_outliers = outliers.sum()

if num_outliers > 0:
    # Remove rows with outliers
    aep_data_cleaned = aep_data_cleaned[~outliers]

    print("Number of Outliers:", num_outliers)
    print("Data cleaned successfully.")
    print("AEP Data Shape (Cleaned):", aep_data_cleaned.shape)
else:
    print("No outliers found. Data is already clean.")

"""# __Preliminary Data Analysis__

Visualization are created to better understand the trends in the dataset.

**Time Series Plot**
"""

plt.figure(figsize=(6, 3))
plt.plot(aep_data_cleaned.index, aep_data_cleaned['AEP_MW'], label='AEP_MW')
plt.title('AEP Hourly Power Consumption')
plt.xlabel('Datetime')
plt.ylabel('Power Consumption (MW)')
#plt.legend()
plt.grid(True)
plt.show()

"""**Histogram Plot**"""

plt.figure(figsize=(6, 3))
plt.hist(aep_data_cleaned['AEP_MW'], bins=30, edgecolor='k', alpha=0.7)
plt.title('Distribution of Power Consumption')
plt.xlabel('Power Consumption (MW)')
plt.ylabel('Frequency')
plt.grid(True)
plt.show()

"""# __Feature Extraction__


-  In this section , we extract various features from the 'Datetime' index, such as the hour of the day, the day of the week, the quarter of the year, the month, the year, the day of the year, and the week of the year. These features are added as new columns in the DataFrame. Additionally, a 'season' feature is created based on the 'month' feature.
"""

aep_data_cleaned = aep_data_cleaned.copy()
# Feature Engineering
aep_data_cleaned['hour'] = aep_data_cleaned.index.hour
aep_data_cleaned['dayofweek'] = aep_data_cleaned.index.dayofweek
aep_data_cleaned['quarter'] = aep_data_cleaned.index.quarter
aep_data_cleaned['month'] = aep_data_cleaned.index.month
aep_data_cleaned['year'] = aep_data_cleaned.index.year
aep_data_cleaned['dayofyear'] = aep_data_cleaned.index.dayofyear
aep_data_cleaned['dayofmonth'] = aep_data_cleaned.index.day
aep_data_cleaned['weekofyear'] = aep_data_cleaned.index.isocalendar().week
aep_data_cleaned['season'] = aep_data_cleaned['month'] % 12 // 3 + 1

# Print the updated DataFrame to CSV
print(aep_data_cleaned.head())
aep_data_cleaned.to_csv('AEP_hourly_cleaned.csv')
print("Feature Extraction Completed Successfully")

"""# __Exploratory Data Analysis__


-  The code performs exploratory data analysis on the cleaned and preprocessed DataFrame. It prints the first few rows of the DataFrame, the basic statistics of the numerical columns, the shape of the DataFrame, and information about the DataFrame. Additionally, it visualizes the data using box plots and heatmaps to understand the distribution and correlation of different features


Note : The seasons follow this numerical representation


1. Spring
2. Summer
3. Autumn
4. Winter
"""

print("First few rows of the dataset:")
print(aep_data_cleaned.head())

print("\n\nBasic statistics of numerical columns:")
print(aep_data_cleaned.describe())

print("\n\nDataset shape (rows, columns):\n", aep_data_cleaned.shape)

print("\n\nInformation about the dataset:")
print(aep_data_cleaned.info())

"""**Box Plot**"""

# Create a copy of the DataFrame
aep_data_copy = aep_data_cleaned.copy()

# Plot without changing the original DataFrame
plt.figure(figsize=(10, 6))
sns.boxplot(x='dayofweek', y='AEP_MW', data=aep_data_copy)
plt.title('Power Consumption by Day of the Week')
plt.xlabel('Day of the Week')
plt.ylabel('Power Consumption (MW)')
plt.xticks(rotation=45)
plt.grid(True)
plt.show()


# Create a copy of the DataFrame
aep_data_copy = aep_data_cleaned.copy()

# Add the 'MonthOfYear' column to the copy
aep_data_copy['monthofyear'] = aep_data_cleaned.index.month_name()

# Plot without changing the original DataFrame
plt.figure(figsize=(10, 6))
sns.boxplot(x='monthofyear', y='AEP_MW', data=aep_data_copy)
plt.title('Power Consumption by Month of the Year')
plt.xlabel('Month of the Year')
plt.ylabel('Power Consumption (MW)')
plt.xticks(rotation=45)
plt.grid(True)
plt.show()

"""**Plotting Seasonality in the Data**"""

aep_data_cleaned = pd.read_csv('AEP_hourly_cleaned.csv', parse_dates=True, index_col='Datetime')# Daily
plt.figure(figsize=(10, 6))
plt.plot(aep_data_cleaned.resample('D')['AEP_MW'].mean())
plt.title('Daily Average Energy Consumption')
plt.xlabel('Datetime')
plt.ylabel('AEP_MW')
plt.show()

# Weekly
plt.figure(figsize=(10, 6))
plt.plot(aep_data_cleaned.resample('W')['AEP_MW'].mean())
plt.title('Weekly Average Energy Consumption')
plt.xlabel('Datetime')
plt.ylabel('AEP_MW')
plt.show()

# Monthly
plt.figure(figsize=(10, 6))
plt.plot(aep_data_cleaned.resample('M')['AEP_MW'].mean())
plt.title('Monthly Average Energy Consumption')
plt.xlabel('Datetime')
plt.ylabel('AEP_MW')
plt.show()

# Yearly
plt.figure(figsize=(10, 6))
plt.plot(aep_data_cleaned.resample('Y')['AEP_MW'].mean())
plt.title('Yearly Average Energy Consumption')
plt.xlabel('Datetime')
plt.ylabel('AEP_MW')
plt.show()

"""# **Visualizing the Features**"""

plt.figure(figsize=(5, 4))
corr_matrix = aep_data_cleaned.corr()
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Correlation Heatmap')
plt.show()

print("\n\n")

plt.figure(figsize=(5, 4))
sns.boxplot(x='season', y='AEP_MW', data=aep_data_cleaned, palette='coolwarm')
plt.title('Energy Consumption by Season')
plt.xlabel('Season')
plt.ylabel('Energy Consumption (AEP_MW)')
plt.show()

"""Note : The seasons follow this numerical representation


1. Spring
2. Summer
3. Autumn
4. Winter

# __Data Preprocessing for Time Series Analysis: Normalization and Time Series Splitting__

**Data Loading and Preprocessing for Time Series Analysis**

- Here we load the cleaned and preprocessed DataFrame from a CSV file. The 'Datetime' column is converted to datetime format using the pd.to_datetime() function. The 'Datetime' column is set as the index of the DataFrame using the set_index() function. The DataFrame is sorted by 'Datetime' using the sort_index() function
"""

# Load cleaned CSV file
aep_data_cleaned = pd.read_csv('AEP_hourly_cleaned.csv')

# Convert 'Datetime' to datetime format if necessary
aep_data_cleaned['Datetime'] = pd.to_datetime(aep_data_cleaned['Datetime'])

# Set 'Datetime' as the index
aep_data_cleaned.set_index('Datetime', inplace=True)

# Sort the DataFrame by 'Datetime'
aep_data_cleaned.sort_index(inplace=True)

print("Data Processed Successfully")

"""**Data Normalization**

The data is then normalized using the MinMaxScaler from sklearn.preprocessing. This scales the data to a range between 0 and 1. This is important for many machine learning algorithms that perform time series forecasting.
"""

# Initialize a scaler
scaler = MinMaxScaler()

# Fit and transform the data
aep_data_cleaned = pd.DataFrame(scaler.fit_transform(aep_data_cleaned), columns=aep_data_cleaned.columns)
print("The data has been Normalized successfully")
print(aep_data_cleaned.head())

"""__Time Series Split and Sequencing__


- In this section, we set up a time series split for our data. We define a sequence length of 24, which means that our model will consider 24 time steps in the past to predict the next time step. We also initialize a TimeSeriesSplit object with 5 splits.

- We then separate our target variable 'AEP_MW' from the rest of the dataset. The target variable is what we aim to predict, and the rest of the dataset is our features or predictors.

- Next, we iterate through the splits, separating our data into training and testing sets. We then reshape our data according to the sequence length we defined earlier. This is done by sliding a window of length sequence_length over our data and appending the result to a list. We then convert these lists to numpy arrays for easier manipulation later on.
"""

# Define the sequence length
sequence_length = 24  # You can adjust this based on your needs (e.g., 24 for daily patterns)
n_splits = 5
# Initialize the TimeSeriesSplit
tscv = TimeSeriesSplit(n_splits=n_splits)

y = aep_data_cleaned['AEP_MW'].values
X = aep_data_cleaned.drop('AEP_MW', axis=1).values

# Initialize lists to store training and testing data
X_train_list = []
X_test_list = []
y_train_list = []
y_test_list = []

# Iterate through the splits
for train_index, test_index in tscv.split(X):
    X_train, X_test = X[train_index], X[test_index]
    y_train, y_test = y[train_index], y[test_index]

    # Reshape the data with the defined sequence length
    for i in range(0, len(X_train) - sequence_length + 1):
        X_train_list.append(X_train[i:i+sequence_length])
        y_train_list.append(y_train[i+sequence_length-1])
    for i in range(0, len(X_test) - sequence_length + 1):
        X_test_list.append(X_test[i:i+sequence_length])
        y_test_list.append(y_test[i+sequence_length-1])

# Convert lists to numpy arrays
X_train = np.array(X_train_list)
X_test = np.array(X_test_list)
y_train = np.array(y_train_list)
y_test = np.array(y_test_list)
print("Data Has been Succesfully Split and Timesteps Have been Added")

"""Shape checking"""

print("Shape of X_train:", X_train.shape)
print("Shape of y_train:", y_train.shape)
print("Shape of X_test:", X_test.shape)
print("Shape of y_test:", y_test.shape)

"""# __LSTM Model Architecture and Training for Time Series Forecasting__


- In this section, we define and train our LSTM model. The model is a sequential model, meaning that the layers are stacked on top of each other. The model consists of several bidirectional LSTM layers, each followed by a dropout layer for regularization. The final layer is a dense layer with a single unit, which will output our predicted value.

- We compile our model with the Adam optimizer and a learning rate of 0.001. We use mean squared error as our loss function since this is a regression problem.

- We then fit our model to our training data. We use a batch size of 64 and run for 25 epochs. We also specify a validation split of 0.2, meaning that 20% of our training data will be used for validation. We also use several callbacks, including ModelCheckpoint to save the best model, ReduceLROnPlateau to reduce the learning rate when the validation loss stops improving, and EarlyStopping to stop training early if the validation loss stops improving for a certain number of epochs.
"""

# Initialize the model
model = Sequential()

# Add the first Bidirectional LSTM layer
model.add(Bidirectional(LSTM(units=64, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2]))))
model.add(Dropout(0.2))  # Dropout for regularization

# Add 4 more Bidirectional LSTM layers using a for loop
for i in range(4):
    model.add(Bidirectional(LSTM(units=64, return_sequences=True)))
    model.add(Dropout(0.2))  # Dropout for regularization

# Add the output layer
model.add(Bidirectional(LSTM(units=64, return_sequences=False)))
model.add(Dropout(0.2))
model.add(Dense(units=1))

# Compile the model
optimizer = Adam(learning_rate=0.001)
model.compile(optimizer=optimizer, loss='mean_squared_error')
early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

callbacks = [
     ModelCheckpoint("/content/drive/MyDrive/NM Proj/save.h5", verbose=1, save_best_only=True, save_weights_only=False,),
     ReduceLROnPlateau(monitor="val_loss", patience=3, factor=0.1, verbose=1, min_lr=1e-9),
     EarlyStopping(monitor="val_loss", patience=10, verbose=1)
]
# Train the model
history = model.fit(
    X_train,
    y_train,
    epochs=25,
    batch_size=64,
    validation_split=0.2,
    callbacks=callbacks

)
model.summary()
print("Training has been Completed")

"""**LSTM Autoencoder for Anomaly Detection**

- In this section, we create an LSTM autoencoder for anomaly detection. An autoencoder is a type of neural network that is trained to copy its input to its output. It can be used to learn a compressed representation of the input data, and is often used for anomaly detection.

- We define our autoencoder with an encoder part, consisting of two LSTM layers, and a decoder part, which mirrors the encoder. We compile the model with the Adam optimizer and mean squared error loss, and fit it to our training data.
"""

# Creating copy of X_train for the autoencoder model
X_train_copy = X_train.copy()

# LSTM Autoencoder
inputs = Input(shape=(X_train_copy.shape[1], X_train_copy.shape[2]))
encoded = LSTM(64, return_sequences=True)(inputs)
encoded = LSTM(32, return_sequences=False)(encoded)

decoded = RepeatVector(X_train_copy.shape[1])(encoded)
decoded = LSTM(32, return_sequences=True)(decoded)
decoded = LSTM(X_train_copy.shape[2], return_sequences=True)(decoded)

# Defining the autoencoder model
autoencoder = Model(inputs, decoded)

# Compile the model
autoencoder.compile(optimizer='adam', loss='mse')

# Fit the model
autoencoder.fit(X_train_copy, X_train_copy, epochs=20, batch_size=64, validation_split=0.1)

"""# __Model Evaluation and Performance Visualization for Time Series Forecasting__


In this section, we evaluate the performance of our model. We plot the training and validation loss over each epoch to visualize how our model learned over time. We then make predictions on our testing data and calculate several metrics, including the mean absolute error, mean squared error, root mean squared error, and R-squared score. We plot the actual vs. predicted values to visually assess the performance of our model.

**Training History**
"""

# Get the training history
loss = history.history['loss']
val_loss = history.history['val_loss']
epochs = range(1, len(loss) + 1)

# Plot the training and validation loss
plt.figure(figsize=(5, 3))
plt.plot(epochs, loss, 'bo', label='Training Loss')
plt.plot(epochs, val_loss, 'r', label='Validation Loss')
plt.title('Training and Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.grid(True)
plt.show()

"""**Evaluating Performance on the Testing Set**"""

# Prediction
y_pred = model.predict(X_test)

# Calculate the mean absolute error
mae = mean_absolute_error(y_test, y_pred)
print('Mean Absolute Error:', mae)

# Calculate the Mean Squared Error
mse = mean_squared_error(y_test, y_pred)
rmse = math.sqrt(mse)
print("Mean Squared Error (MSE):", mse)
print("Root Mean Squared Error (RMSE):", rmse)

# Calculate the R-squared score
r2 = r2_score(y_test, y_pred)
print('R-squared (R2) Score:', r2)

# Plotting Actual vs. Predicted Values
plt.plot(y_test, label='Actual Values', color='blue')
plt.plot(y_pred, label='Predicted Values', color='red')
plt.xlabel('Time')
plt.ylabel('Value')
plt.legend()
plt.title('Actual vs Predicted Values')
plt.show()

"""**Evaluating Performance Over the Entire Dataset**"""

X = np.concatenate((X_train, X_test), axis=0)
y = np.concatenate((y_train, y_test), axis=0)

# Prediction
y_pred = model.predict(X)

# Calculate the mean absolute error
mae = mean_absolute_error(y, y_pred)
print('Mean Absolute Error:', mae)

# Calculate the Mean Squared Error
mse = mean_squared_error(y, y_pred)
rmse = math.sqrt(mse)
print("Mean Squared Error (MSE):", mse)
print("Root Mean Squared Error (RMSE):", rmse)

# Calculate the R-squared score
r2 = r2_score(y, y_pred)
print('R-squared (R2) Score:', r2)

# Plotting Actual vs. Predicted Values
plt.plot(y, label='Actual Values', color='blue')
plt.plot(y_pred, label='Predicted Values', color='red')
plt.xlabel('Time')
plt.ylabel('Value')
plt.legend()
plt.title('Actual vs Predicted Values')
plt.show()

"""# __Anomaly Detection__

- Finally, we used our trained autoencoder for anomaly detection. We made predictions on our testing data and calculated the mean squared error between the actual and predicted values. We then defined a threshold for anomalies as the 99.9th percentile of the mean squared error. Any data point with a mean squared error greater than this threshold was considered an anomaly. We printed the indices of these anomalous data points.

"""

# Using the autoencoder to make predictions on the test data
X_test_copy = X_test.copy()
X_test_pred = autoencoder.predict(X_test_copy)

# Calculating the mean squared error (MSE) between the actual and predicted values
mse = np.mean(np.power(X_test - X_test_pred, 2), axis=(1, 2))

# Defining a threshold for anomaly detection
threshold = np.quantile(mse, 0.999)

# Detecting anomalies in the test data
anomalies = mse > threshold

# Printing the indices of anomalous data points
anomalous_indices = np.where(anomalies)[0]
print("Anomalous Data Points (Indices):", anomalous_indices)